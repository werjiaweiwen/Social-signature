{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f6680c",
   "metadata": {},
   "source": [
    "User identification scenario One: ranking situationFirst, calculate the required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f23dd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"G:\\\\data_all\\\\mathoverflow_a2q\\\\filtered_results.csv\")\n",
    "ego1 = list(df['ego'])\n",
    "print(len(ego1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00bf1703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import *\n",
    "ego_unique=ego1\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "df1 = pd.read_excel('G:\\\\data_all\\\\mathoverflow_a2q\\\\M\\\\M_data_1.xlsx')\n",
    "df2 = pd.read_excel('G:\\\\data_all\\\\mathoverflow_a2q\\\\M\\\\M_data_2.xlsx')\n",
    "df3 = pd.read_excel('G:\\\\data_all\\\\mathoverflow_a2q\\\\M\\\\M_data_3.xlsx')\n",
    "df4 = pd.read_excel('G:\\\\data_all\\\\mathoverflow_a2q\\\\M\\\\M_data_4.xlsx')\n",
    "df5 = pd.read_excel('G:\\\\data_all\\\\mathoverflow_a2q\\\\M\\\\M_data_5.xlsx')\n",
    "df6 = pd.read_excel('G:\\\\data_all\\\\mathoverflow_a2q\\\\M\\\\M_data_6.xlsx')\n",
    "\n",
    "w_list_list1 = []\n",
    "grouped1 = df1.groupby([\"ego\"])\n",
    "def get_w_list1(user1):\n",
    "\n",
    "    group1 = grouped1.get_group(user1)\n",
    "\n",
    "    w = group1['weight']\n",
    "    user_w = []\n",
    "    fl = []\n",
    "    for x in w:\n",
    "        user_w.append(x)\n",
    "        user_w.sort(reverse=True)\n",
    "        #eturn user_w\n",
    "    total = sum(user_w)\n",
    "    if(total!=0):\n",
    "        for x in user_w:\n",
    "            x = x / total\n",
    "            fl.append(x)\n",
    "    w_list_list1.append(fl)\n",
    "    \n",
    "w_list_list2 = []\n",
    "grouped2 = df2.groupby([\"ego\"])\n",
    "def get_w_list2(user1):\n",
    "\n",
    "    group2 = grouped2.get_group(user1)\n",
    "\n",
    "    w = group2['weight']\n",
    "    user_w = []\n",
    "    f2 = []\n",
    "    for x in w:\n",
    "        user_w.append(x)\n",
    "        user_w.sort(reverse=True)\n",
    "        #eturn user_w\n",
    "    total = sum(user_w)\n",
    "    if(total!=0):\n",
    "        for x in user_w:\n",
    "            x = x / total\n",
    "            f2.append(x)\n",
    "    w_list_list2.append(f2)\n",
    "    \n",
    "w_list_list3 = []\n",
    "grouped3 = df3.groupby([\"ego\"])\n",
    "def get_w_list3(user1):\n",
    "\n",
    "    group3 = grouped3.get_group(user1)\n",
    "\n",
    "    w = group3['weight']\n",
    "    user_w = []\n",
    "    f3 = []\n",
    "    for x in w:\n",
    "        user_w.append(x)\n",
    "        user_w.sort(reverse=True)\n",
    "        #eturn user_w\n",
    "    total = sum(user_w)\n",
    "    if(total!=0):\n",
    "        for x in user_w:\n",
    "            x = x / total\n",
    "            f3.append(x)\n",
    "    w_list_list3.append(f3)\n",
    "    \n",
    "w_list_list4 = []\n",
    "grouped4 = df4.groupby([\"ego\"])\n",
    "def get_w_list4(user1):\n",
    "\n",
    "    group4 = grouped4.get_group(user1)\n",
    "\n",
    "    w = group4['weight']\n",
    "    user_w = []\n",
    "    f4 = []\n",
    "    for x in w:\n",
    "        user_w.append(x)\n",
    "        user_w.sort(reverse=True)\n",
    "        #eturn user_w\n",
    "    total = sum(user_w)\n",
    "    if(total!=0):\n",
    "        for x in user_w:\n",
    "            x = x / total\n",
    "            f4.append(x)\n",
    "    w_list_list4.append(f4)\n",
    "    \n",
    "w_list_list5 = []\n",
    "grouped5 = df5.groupby([\"ego\"])\n",
    "def get_w_list5(user1):\n",
    "\n",
    "    group5 = grouped5.get_group(user1)\n",
    "\n",
    "    w = group5['weight']\n",
    "    user_w = []\n",
    "    f5 = []\n",
    "    for x in w:\n",
    "        user_w.append(x)\n",
    "        user_w.sort(reverse=True)\n",
    "        #eturn user_w\n",
    "    total = sum(user_w)\n",
    "    if(total!=0):\n",
    "        for x in user_w:\n",
    "            x = x / total\n",
    "            f5.append(x)\n",
    "    w_list_list5.append(f5)\n",
    "    \n",
    "w_list_list6 = []\n",
    "grouped6 = df6.groupby([\"ego\"])\n",
    "def get_w_list6(user1):\n",
    "\n",
    "    group6 = grouped6.get_group(user1)\n",
    "\n",
    "    w = group6['weight']\n",
    "    user_w = []\n",
    "    f6 = []\n",
    "    for x in w:\n",
    "        user_w.append(x)\n",
    "        user_w.sort(reverse=True)\n",
    "        #eturn user_w\n",
    "    total = sum(user_w)\n",
    "    if(total!=0):\n",
    "        for x in user_w:\n",
    "            x = x / total\n",
    "            f6.append(x)\n",
    "    w_list_list6.append(f6)\n",
    "    \n",
    "for x in ego_unique:\n",
    "    get_w_list1(x)\n",
    "for x in ego_unique:\n",
    "    get_w_list2(x)\n",
    "for x in ego_unique:\n",
    "    get_w_list3(x)\n",
    "for x in ego_unique:\n",
    "    get_w_list4(x)\n",
    "for x in ego_unique:\n",
    "    get_w_list5(x)\n",
    "for x in ego_unique:\n",
    "    get_w_list6(x)\n",
    "\n",
    "# the final d_list to show\n",
    "test_d = []\n",
    "\n",
    "\n",
    "class JSD:\n",
    "\n",
    "    def H(self, p):\n",
    "        return -sum(_p * log(_p if _p > 0 else 1) for _p in p)\n",
    "\n",
    "    def JSD_d(self, p, q):\n",
    "        lp = len(p)\n",
    "        lq = len(q)\n",
    "        d_value = lp - lq\n",
    "        if(d_value > 0):\n",
    "            i = 1\n",
    "            for i in range(d_value):\n",
    "                q.append(0)\n",
    "        else:\n",
    "            for i in range(-d_value):\n",
    "                p.append(0)\n",
    "\n",
    "        M = [0.5 * (_p + _q) for _p, _q in zip(p, q)]\n",
    "        t = (self.H(M) - (self.H(p) + self.H(q)) / 2)\n",
    "        #return round(t,3)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff57603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsd = JSD()\n",
    "len_ego = len(ego_unique)\n",
    "test_d = []\n",
    "\n",
    "list_names = [w_list_list1, w_list_list2, w_list_list3, w_list_list4, w_list_list5, w_list_list6]\n",
    "#list_names = [w_list_list1, w_list_list2, w_list_list3]\n",
    "\n",
    "for t in range(len(list_names)):\n",
    "    for z in range(len(list_names)):\n",
    "        for i in range(0, len_ego):\n",
    "            for j in range(0, len_ego):\n",
    "                if t != z:\n",
    "                    w_list_1 = list_names[t]\n",
    "                    w_list_2 = list_names[z]\n",
    "                    ego_1 = ego_unique[i]\n",
    "                    ego_2 = ego_unique[j]\n",
    "                    d = jsd.JSD_d(w_list_1[i], w_list_2[j])\n",
    "                    test_d.append({'ego_1': ego_1, 'ego_2': ego_2, 'JSD_value': d})\n",
    "\n",
    "dref = pd.DataFrame(test_d)\n",
    "dref.to_csv(\"G:\\\\data_all\\\\mathoverflow_a2q\\\\M\\\\nearest_known_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8416b098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"G:\\\\data_all\\\\mathoverflow_a2q\\\\M\\\\nearest_known_labels.csv\")\n",
    "\n",
    "# For each pair (ego1, ego2), we need to create a new column for each JSD_value\n",
    "# Generate a sequence number using cumcount and a new column name for each JSD_value of each pair (ego1, ego2)\n",
    "df['count'] = df.groupby(['ego_1', 'ego_2']).cumcount() + 1\n",
    "df_pivot = df.pivot(index=['ego_1', 'ego_2'], columns='count', values='JSD_value')\n",
    "\n",
    "# Rename columns to represent their contents more clearly\n",
    "df_pivot.columns = [f'JSD_value_{i}' for i in df_pivot.columns]\n",
    "\n",
    "df_pivot.reset_index(inplace=True)\n",
    "\n",
    "df_pivot.to_csv('G:\\\\data_all\\\\mathoverflow_a2q\\\\M\\\\nearest_known_labels_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff308f3",
   "metadata": {},
   "source": [
    "Ranking-based User Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e2c04965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read a table with known user tags, unknown user tags, and social fingerprint distances\n",
    "#df = pd.read_excel('F:\\\\shiyan2\\\\0923\\\\email\\\\R\\\\identification\\\\nearest_data.xlsx')\n",
    "df = pd.read_excel('G:\\\\data_all\\\\mathoverflow_a2q\\\\T\\\\nearest_data.xlsx')\n",
    "# Define a function to determine whether the recognition was successful\n",
    "def is_recognized(nearest_known_labels, true_label, n=1):\n",
    "    # Capture the last n known user tags\n",
    "    nearest_known_labels = nearest_known_labels[:n]\n",
    "    # Determine if the correct known user tag exists\n",
    "    return true_label in nearest_known_labels\n",
    "\n",
    "# Initialize the variable to record the total number of users and the number of successfully identified users\n",
    "total_users = 0\n",
    "recognized_users = 0\n",
    "\n",
    "# Store recognition accuracy for different n values\n",
    "results = []\n",
    "\n",
    "n_values = list(range(1, 55))\n",
    "\n",
    "grouped = df.groupby('未知用户预设标签')\n",
    "\n",
    "for n in n_values:\n",
    "    total_users = 0\n",
    "    recognized_users = 0\n",
    "\n",
    "    for label, group in grouped:\n",
    "        true_label = label  # Unknown user default label\n",
    "        known_labels = group['已知用户标签'].tolist()  # List of known user tags\n",
    "        distances = group['社交指纹距离'].tolist()  # Social signature distance list\n",
    "\n",
    "        # Find the n known user tags with the smallest distance\n",
    "        nearest_known_labels = [known_labels[i] for i in sorted(range(len(distances)), key=lambda i: distances[i])[:n]]\n",
    "\n",
    "        # Check whether the identification is successful\n",
    "        if is_recognized(nearest_known_labels, true_label, n):\n",
    "            recognized_users += 1\n",
    "\n",
    "        total_users += 1\n",
    "\n",
    "    # Computational recognition accuracy\n",
    "    accuracy = round(recognized_users / total_users,3)\n",
    "\n",
    "    results.append({'n': n, '识别准确率': accuracy})\n",
    "\n",
    "result_df = pd.DataFrame(results)\n",
    "\n",
    "result_df.to_excel('G:\\\\data_all\\\\mathoverflow_a2q\\\\MU\\\\recognition_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6321805",
   "metadata": {},
   "source": [
    "Improved version, converts nearest_known_labels to the final result directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9d44c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('G:\\\\data_all\\\\mathoverflow_a2q\\\\N\\\\nearest_known_labels_1.csv')\n",
    "\n",
    "def is_recognized(nearest_known_labels, true_label, n=1):\n",
    "    nearest_known_labels = nearest_known_labels[:n]\n",
    "    return true_label in nearest_known_labels\n",
    "\n",
    "\n",
    "total_users = 0\n",
    "recognized_users = 0\n",
    "\n",
    "n_values = list(range(1, 9))\n",
    "\n",
    "distance_columns = [col for col in df.columns if col.startswith('JSD_value')]\n",
    "\n",
    "result_df = pd.DataFrame({'n': n_values})\n",
    "\n",
    "for distance_col in distance_columns:\n",
    "    results = []\n",
    "    grouped = df.groupby('ego_2')\n",
    "\n",
    "    for n in n_values:\n",
    "        total_users = 0\n",
    "        recognized_users = 0\n",
    "\n",
    "        for label, group in grouped:\n",
    "            true_label = label\n",
    "            known_labels = group['ego_1'].tolist() \n",
    "            distances = group[distance_col].tolist()  \n",
    "\n",
    "            nearest_known_labels = [known_labels[i] for i in sorted(range(len(distances)), key=lambda i: distances[i])[:n]]\n",
    "\n",
    "            if is_recognized(nearest_known_labels, true_label, n):\n",
    "                recognized_users += 1\n",
    "\n",
    "            total_users += 1\n",
    "\n",
    "        accuracy = round(recognized_users / total_users, 3)\n",
    "        results.append(accuracy)\n",
    "\n",
    "    result_df[distance_col] = results\n",
    "\n",
    "result_df.to_excel('G:\\\\data_all\\\\mathoverflow_a2q\\\\N\\\\recognition_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb41c49",
   "metadata": {},
   "source": [
    "Threshold based user identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f7c9db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel('G:\\\\data_all\\\\mathoverflow_a2q\\\\all_data.xlsx')\n",
    "true_labels = data.iloc[:, -1]  # The last column is the label\n",
    "\n",
    "# Gets the names of all feature columns\n",
    "feature_columns = data.loc[:, 'N':'N']  \n",
    "\n",
    "# Create a list of thresholds between 0 and 1 with a step of 0.01\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "#thresholds = np.arange(0, 0.04, 0.0001)\n",
    "#thresholds = np.arange(0, 0.01, 0.0001)\n",
    "#thresholds = np.arange(0, 0.025, 0.0002)\n",
    "\n",
    "#Create a dictionary to store thresholds and recognition success rates for each feature column\n",
    "feature_success_rates = {}\n",
    "\n",
    "# Repeat 1000 times\n",
    "for i in range(1000):\n",
    "    sampled_data = data.copy()  # Copy the original data for repeated sampling\n",
    "    for column in feature_columns:\n",
    "        # Randomly select the same number of negative samples as positive samples\n",
    "        negative_samples = sampled_data[sampled_data.iloc[:, -1] == 0].sample(n=len(sampled_data[sampled_data.iloc[:, -1] == 1]), random_state=i)\n",
    "        sampled_data = pd.concat([sampled_data[sampled_data.iloc[:, -1] == 1], negative_samples])\n",
    "\n",
    "        success_rates = []\n",
    "        for threshold in thresholds:\n",
    "            predicted_labels = [0 if x > threshold else 1 for x in sampled_data[column]]\n",
    "            correct_predictions = sum(1 for pred, true in zip(predicted_labels, sampled_data.iloc[:, -1]) if pred == true)\n",
    "            success_rate = correct_predictions / len(sampled_data)\n",
    "            success_rates.append(success_rate)\n",
    "        feature_success_rates[f'{column}_sampled_{i}'] = success_rates\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "result_df = pd.DataFrame(feature_success_rates)\n",
    "\n",
    "# Add threshold column\n",
    "result_df['阈值'] = thresholds\n",
    "\n",
    "# Calculate the average of each row\n",
    "row_means = result_df.iloc[:, :-1].mean(axis=1)\n",
    "# Calculate the variance of each row\n",
    "row_variances = result_df.iloc[:, :-1].var(axis=1)\n",
    "\n",
    "result_df['平均值'] = row_means\n",
    "result_df['方差'] = row_variances\n",
    "\n",
    "result_df.to_excel('G:\\\\data_all\\\\mathoverflow_a2q\\\\threshold_success_rates\\\\N.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9141bc",
   "metadata": {},
   "source": [
    "Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5009f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#df = pd.read_excel('G:\\\\data_all\\\\mathoverflow_c2a\\\\all_data.xlsx')\n",
    "df = pd.read_excel('G:\\\\data_all\\\\mathoverflow_a2q\\\\all_data.xlsx')\n",
    "\n",
    "num_iterations = 100\n",
    "\n",
    "accuracy_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Initializes the list of importance of stored features\n",
    "feature_importances = []\n",
    "\n",
    "# Cycle of random sampling and model training\n",
    "for _ in range(num_iterations):\n",
    "    # Randomly select the same number of negative samples as positive samples\n",
    "    positive_samples = df[df['leibie'] == 1]\n",
    "    negative_samples = df[df['leibie'] == 0].sample(n=len(positive_samples), replace=True)\n",
    "    balanced_data = pd.concat([positive_samples, negative_samples])\n",
    "    \n",
    "    # Divide the training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        balanced_data.loc[:, 'N':'A'], \n",
    "        balanced_data['leibie'], test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    # Use the XGBoost model\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "    xgb_model = XGBClassifier()\n",
    "    grid_search = GridSearchCV(xgb_model, param_grid, cv=3, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Train the XGBoost model with the best parameters\n",
    "    model = XGBClassifier(**best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate accuracy, recall and F1 values\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    recall_scores.append(recall_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "\n",
    "    # Collect feature importance\n",
    "    feature_importances.append(model.feature_importances_)\n",
    "\n",
    "# Get the feature name\n",
    "feature_names = balanced_data.loc[:, 'N':'A'].columns\n",
    "\n",
    "# Average of feature importance\n",
    "mean_feature_importances = np.mean(feature_importances, axis=0)\n",
    "\n",
    "# Create a DataFrame of feature importance for sorting and visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': mean_feature_importances\n",
    "})\n",
    "\n",
    "# In order of importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=True)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, len(importance_df) / 2))  \n",
    "bars = plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "\n",
    "for bar in bars:\n",
    "    plt.text(\n",
    "        bar.get_width(),       \n",
    "        bar.get_y() + bar.get_height() / 2,  \n",
    "        f'{bar.get_width():.3f}',  \n",
    "        va='center'  \n",
    "    )\n",
    "\n",
    "plt.xlabel('F-score (Importance)')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()\n",
    "# Calculate mean, variance, and standard deviation\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "variance_accuracy = np.var(accuracy_scores)\n",
    "std_dev_accuracy = np.std(accuracy_scores)\n",
    "\n",
    "mean_recall = np.mean(recall_scores)\n",
    "variance_recall = np.var(recall_scores)\n",
    "std_dev_recall = np.std(recall_scores)\n",
    "\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "variance_f1 = np.var(f1_scores)\n",
    "std_dev_f1 = np.std(f1_scores)\n",
    "\n",
    "# Output result\n",
    "print(\"Mean Accuracy:\", mean_accuracy)\n",
    "print(\"Variance of Accuracy:\", variance_accuracy)\n",
    "print(\"Standard Deviation of Accuracy:\", std_dev_accuracy)\n",
    "\n",
    "#print(\"Mean Recall:\", mean_recall)\n",
    "#print(\"Variance of Recall:\", variance_recall)\n",
    "#print(\"Standard Deviation of Recall:\", std_dev_recall)\n",
    "\n",
    "#print(\"Mean F1 Score:\", mean_f1)\n",
    "#print(\"Variance of F1 Score:\", variance_f1)\n",
    "#print(\"Standard Deviation of F1 Score:\", std_dev_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082e8416",
   "metadata": {},
   "source": [
    "Time stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54601806",
   "metadata": {},
   "source": [
    "The time stability data is calculated in the sorting case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d5325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a list of folder names\n",
    "folders = ['N', 'R', 'O', 'K', 'M', 'T', 'MU', 'A']\n",
    "\n",
    "# Define column names for each group\n",
    "group_1 = [12, 21, 23, 32, 34, 43, 45, 54, 56, 65]  \n",
    "group_2 = [13, 24, 31, 35, 42, 46, 53, 64]\n",
    "group_3 = [14, 25, 36, 41, 52, 63]\n",
    "group_4 = [15, 26, 51, 62]\n",
    "group_5 = [16, 61]\n",
    "\n",
    "# Separate each set of data and calculate the average of each row\n",
    "groups = [group_1, group_2, group_3, group_4, group_5]\n",
    "group_names = ['rankT1', 'rankT2', 'rankT3', 'rankT4', 'rankT5']\n",
    "\n",
    "# Initializes a dictionary to store a list of data boxes for each group\n",
    "group_dfs = {name: [] for name in group_names}\n",
    "\n",
    "# Process files in each folder\n",
    "for folder in folders:\n",
    "    file_path = f'G:\\\\data_all\\\\mathoverflow_c2q\\\\{folder}\\\\recognition_results.xlsx'\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    for group, name in zip(groups, group_names):\n",
    "        # Check that the column name exists\n",
    "        missing_columns = [col for col in group if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Columns {missing_columns} are not in the dataframe for group {name} in folder {folder}.\")\n",
    "            continue\n",
    "        \n",
    "        # Select the corresponding column\n",
    "        df_group = df[group]\n",
    "        \n",
    "        # Calculate the average of each row and add to the last column\n",
    "        df_group['mean'] = df_group.mean(axis=1)\n",
    "        \n",
    "        # Save to a new Excel file\n",
    "        file_name = f'G:\\\\data_all\\\\mathoverflow_c2q\\\\{folder}\\\\{name}.xlsx'\n",
    "        df_group.to_excel(file_name, index=False)\n",
    "        \n",
    "        # Adds the processed data box to the list of the corresponding group\n",
    "        group_dfs[name].append(df_group)\n",
    "    \n",
    "# Concatenate data boxes for each group and save them\n",
    "for name, dfs in group_dfs.items():\n",
    "    if dfs:\n",
    "        df_concat = pd.concat(dfs, axis=1)\n",
    "        concat_file_path = f'G:\\\\data_all\\\\mathoverflow_c2q\\\\{name}_combined.xlsx'\n",
    "        df_concat.to_excel(concat_file_path, index=False)\n",
    "        print(f\"{name}_combined 文件已成功保存。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "44367844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel('G:\\\\data_all\\\\superuser_c2q\\\\all_data.xlsx', sheet_name=\"T1\")\n",
    "true_labels = data.iloc[:, -1]  # The last column is the label\n",
    "\n",
    "feature_columns = data.loc[:, 'M':'M']  \n",
    "\n",
    "#thresholds = np.arange(0, 1.01, 0.05)\n",
    "thresholds = np.arange(0, 0.03, 0.0001)\n",
    "#thresholds = np.arange(0, 0.01, 0.0001)\n",
    "#thresholds = np.arange(0, 0.03, 0.0002)\n",
    "\n",
    "feature_success_rates = {}\n",
    "\n",
    "for i in range(1000):\n",
    "    sampled_data = data.copy()  \n",
    "    for column in feature_columns:\n",
    "        negative_samples = sampled_data[sampled_data.iloc[:, -1] == 0].sample(n=len(sampled_data[sampled_data.iloc[:, -1] == 1]), random_state=i)\n",
    "        sampled_data = pd.concat([sampled_data[sampled_data.iloc[:, -1] == 1], negative_samples])\n",
    "\n",
    "        success_rates = []\n",
    "        for threshold in thresholds:\n",
    "            predicted_labels = [0 if x > threshold else 1 for x in sampled_data[column]]\n",
    "            correct_predictions = sum(1 for pred, true in zip(predicted_labels, sampled_data.iloc[:, -1]) if pred == true)\n",
    "            success_rate = correct_predictions / len(sampled_data)\n",
    "            success_rates.append(success_rate)\n",
    "        feature_success_rates[f'{column}_sampled_{i}'] = success_rates\n",
    "\n",
    "result_df = pd.DataFrame(feature_success_rates)\n",
    "\n",
    "result_df['阈值'] = thresholds\n",
    "\n",
    "row_means = result_df.iloc[:, :-1].mean(axis=1)\n",
    "\n",
    "row_variances = result_df.iloc[:, :-1].var(axis=1)\n",
    "\n",
    "result_df['平均值'] = row_means\n",
    "result_df['方差'] = row_variances\n",
    "\n",
    "result_df.to_excel('G:\\\\data_all\\\\superuser_c2q\\\\M\\\\threshold_T1.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aa66e03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.6957407407407408\n",
      "Variance of Accuracy: 0.005330967078189298\n",
      "Standard Deviation of Accuracy: 0.07301347189518724\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "\n",
    "df = pd.read_excel('G:\\\\data_all\\\\superuser_c2q\\\\all_data.xlsx', sheet_name=\"T1\")\n",
    "\n",
    "num_iterations = 100\n",
    "\n",
    "accuracy_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "shap_values_list = []\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    positive_samples = df[df['leibie'] == 1]\n",
    "    negative_samples = df[df['leibie'] == 0].sample(n=len(positive_samples), replace=True)\n",
    "    balanced_data = pd.concat([positive_samples, negative_samples])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "            balanced_data.loc[:, 'N':'A'], balanced_data.iloc[:, -1], test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "    xgb_model = XGBClassifier()\n",
    "    grid_search = GridSearchCV(xgb_model, param_grid, cv=3, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Train XGBoost model with the best parameters\n",
    "    model = XGBClassifier(**best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "variance_accuracy = np.var(accuracy_scores)\n",
    "std_dev_accuracy = np.std(accuracy_scores)\n",
    "\n",
    "print(\"Mean Accuracy:\", mean_accuracy)\n",
    "print(\"Variance of Accuracy:\", variance_accuracy)\n",
    "print(\"Standard Deviation of Accuracy:\", std_dev_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9175c00",
   "metadata": {},
   "source": [
    "Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6f949f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlUAAAD7CAYAAABDj2EFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAmmElEQVR4nO3de3xU5b3v8c8vhKRaBERMBSaV0mCIAYqaiLi7qbYbqWLx+tJQWi/R46airacbPXS3nqpba2yxRaS71F30WKWkWtFQKakXvLXVYlAumoKpQk2iKFoleAXH3/ljFnFyJWGeZELyfb9e82LWc1+Pi+WPZ61Zy9wdEREREUlNRroHICIiItIbKKgSERERCUBBlYiIiEgACqpEREREAlBQJSIiIhKAgioRERGRABRUiYiIiASgoEqkFzOzLWb2vpm9k/QZHqDNfws1xg70d5WZ3dld/bXHzM4zsz+lexwi0jMpqBLp/b7m7gOSPq+kczBmlpnO/vfWvjpuEek+CqpE+iAzG2Rmi83sVTOrN7NrzaxflPd5M1tlZm+a2RtmtsTMBkd5dwCfBX4frXpdYWbHmVlds/YbV7OilabfmdmdZtYAnNde/x0Yu5vZxWZWY2Y7zOy/ojE/aWYNZnaXmWVFZY8zszoz+89oX7aY2cxm8/BrM9tmZv8wsx+YWUaUd56Z/dnMfmZm/wR+CywCJkX7/nZUbpqZPRv1XWtmVyW1PzIa77lm9nI0hu8n5feLxvZitC9rzCw3yhtjZg+a2T/NbJOZnZVU7yQzq47q1JvZnA7+pxeRLqSgSqRvuh34CMgDjgBOAC6M8gy4HhgOFAC5wFUA7v5N4GU+Wf36cQf7OwX4HTAYWLKH/jviq8BRwDHAFcAtwMxorGOBGUllDwGGAiOAc4FbzCw/yrsZGASMAr4EnAOcn1R3IvASkAN8A5gFPBnt++CozLtRvcHANOBbZnZqs/F+EcgHvgL8XzMriNK/G431JGAgUAq8Z2afBh4EfhP1PQP4bzMrjOotBv7d3Q+I9nfVnqdMRLqagiqR3u8+M3s7+txnZp8BTgQuc/d33f114GdACYC7/93dH3T3D919G/BTEgFHKp509/vc/WMSwUOb/XfQDe7e4O7PA88BD7j7S+6+HVhJIlBLdmW0P48BK4CzopWxs4HvufsOd98C3Ah8M6neK+5+s7t/5O7vtzYQd3/U3Te4+8fuvh5YSsv5utrd33f3dcA64AtR+oXAD9x9kyesc/c3gZOBLe5+W9T3M8A9wJlRvV3A4WY20N3fivJFJM10j4BI73equz+0e8PMjgb6A6+a2e7kDKA2ys8BFgD/ChwQ5b2V4hhqk74f2l7/HfRa0vf3W9k+JGn7LXd/N2n7HyRW4YYCWdF2ct6INsbdKjObCJSRWDHKArKBu5sV25r0/T1gQPQ9F3ixlWYPBSbuvsQYyQTuiL6fAfwAKDOz9cBcd39yT2MVka6llSqRvqcW+BAY6u6Do89Ad999ael6wIHx7j6QxGUvS6rvzdp7F9h/90a0AnRwszLJdfbUf2gHRpfTdvss8ArwBokVn0Ob5dW3Me7WtiFxiW45kOvug0jcd2WtlGtNLfD5NtIfS5qfwdElx28BuPvT7n4KiUuD9wF3dbA/EelCCqpE+hh3fxV4ALjRzAaaWUZ0o/fuS1YHAO8Ab5vZCODyZk28RuIepN1eAD4V3bDdn8QKSnYK/XeFq80sy8z+lcSltbvdPU4iGLnOzA4ws0NJ3OPU3uMbXgNiu2+EjxwA/NPdP4hWAb/eiXH9CvgvMxttCePN7CDgfuAwM/ummfWPPsVmVhDtx0wzG+Tuu4AGIN6JPkWkiyioEumbziFxqaqaxKW93wHDoryrgSOB7STuP1rWrO71wA+ie7TmRPcxXUwiQKgnsXJVR/va6z+0rVEfr5C4SX6Wu2+M8i4lMd6XgD+RWHW6tZ22VgHPA1vN7I0o7WLgGjPbAfxfOrdq9NOo/AMkgqPFwH7uvoPEzfsl0bi3AjfwSbD6TWBL9GvKWSRWE0Ukzcy9tdVsEZF9n5kdB9zp7rE0D0VE+gCtVImIiIgEoKBKREREJABd/hMREREJQCtVIiIiIgEoqBIREREJIG1PVB86dKiPHDkyXd2LiIiIdNiaNWvecPfmDzZuIm1B1ciRI6mqqkpX9yIiIiIdZmb/2FMZXf4TERERCUBBlYiIiEgACqpEREREAlBQJSIiIhKAgioRERGRABRUiYiIiASQtkcqbKjfzsi5K9LVvYiIiPQCW8qmpXsIjbRSJSIiIhKAgioRERGRABRUiYiIiASgoEpEREQkAAVVIiIiIgEoqBIREZF9WmVlJfn5+eTl5VFWVtYif8mSJYwfP57x48dz7LHHsm7duj3WvfvuuyksLCQjI4OqqqoOjSNIUGVmcTNba2bPmdnvzWxwiHZFRERE2hOPx5k9ezYrV66kurqapUuXUl1d3aTM5z73OR577DHWr1/PlVdeyUUXXbTHumPHjmXZsmVMnjy5w2MJtVL1vrtPcPexwD+B2YHaFREREWnT6tWrycvLY9SoUWRlZVFSUkJFRUWTMsceeywHHnggAMcccwx1dXV7rFtQUEB+fn6nxtIVl/+eBEZ0QbsiIiIiTdTX15Obm9u4HYvFqK+vb7P84sWLOfHEE/eq7p4EfaK6mfUDvgIsbiP/IuAigH4DDw7ZtYiIiPRB7t4izcxaLfvII4+wePFi/vSnP3W6bkeEWqnaz8zWAm8CQ4AHWyvk7re4e5G7F/Xbf1CgrkVERKSvisVi1NbWNm7X1dUxfPjwFuXWr1/PhRdeSEVFBQcddFCn6nZU0HuqgEOBLHRPlYiIiHSD4uJiampq2Lx5Mzt37qS8vJzp06c3KfPyyy9z+umnc8cdd3DYYYd1qm5nBL385+7bzezbQIWZ/cLdd4VsX0RERCRZZmYmCxcuZOrUqcTjcUpLSyksLGTRokUAzJo1i2uuuYY333yTiy++uLFOVVVVm3UB7r33Xi699FK2bdvGtGnTAEbvaSzW2vXEzjKzd9x9QNL274G73P2OtupkDxvtw86dn3LfIiIi0ndtKZvWLf2Y2Rp3L2qvTJCVquSAKtr+Woh2RURERPYVeqK6iIiISAAKqkREREQCUFAlIiIiEoCCKhEREZEAFFSJiIiIBBD0OVWdMW7EIKq66WeQIiIiIl1NK1UiIiIiASioEhEREQlAQZWIiIhIAAqqRERERAJI243qG+q3M3LuinR1LyIi0q7ueqec9B5aqRIREREJQEGViIiISAAKqkREREQCUFAlIiIiEoCCKhERkXZUVlaSn59PXl4eZWVlLfI3btzIpEmTyM7OZt68eU3y3n77bc4880zGjBlDQUEBTz75JAB33303hYWFZGRkUFVV1S37IV0vWFBlZu8kfT/JzGrM7LOh2hcREelu8Xic2bNns3LlSqqrq1m6dCnV1dVNygwZMoQFCxYwZ86cFvW/853v8NWvfpWNGzeybt06CgoKABg7dizLli1j8uTJ3bIf0j2Cr1SZ2VeAm4GvuvvLodsXERHpLqtXryYvL49Ro0aRlZVFSUkJFRUVTcrk5ORQXFxM//79m6Q3NDTw+OOPc8EFFwCQlZXF4MGDASgoKCA/P79b9kG6T9Cgysz+FfgfYJq7vxiybRERke5WX19Pbm5u43YsFqO+vr5DdV966SUOPvhgzj//fI444gguvPBC3n333a4aqvQAIYOqbKACONXdNwZsV0REJC3cvUWamXWo7kcffcQzzzzDt771LZ599lk+/elPt3pPlvQeIYOqXcBfgAvaKmBmF5lZlZlVxd/bHrBrERGR8GKxGLW1tY3bdXV1DB8+vMN1Y7EYEydOBODMM8/kmWee6ZJxSs8QMqj6GDgLKDaz/2ytgLvf4u5F7l7Ub/9BAbsWEREJr7i4mJqaGjZv3szOnTspLy9n+vTpHap7yCGHkJuby6ZNmwB4+OGHOfzww7tyuJJmQd/95+7vmdnJwBNm9pq7Lw7ZvoiISHfKzMxk4cKFTJ06lXg8TmlpKYWFhSxatAiAWbNmsXXrVoqKimhoaCAjI4P58+dTXV3NwIEDufnmm5k5cyY7d+5k1KhR3HbbbQDce++9XHrppWzbto1p06YxYcIE/vjHP6ZzVyUAa+168V41ZPaOuw+IvucCjwOXuXtFa+Wzh432YefOD9K3iIhIaHqhsiQzszXuXtRemWArVbsDquh7LfC5UG2LiIiI9HR6orqIiIhIAAqqRERERAJQUCUiIiISgIIqERERkQAUVImIiIgEEPQ5VZ0xbsQgqvRzVREREekltFIlIiIiEoCCKhEREZEAFFSJiIiIBKCgSkRERCSAtN2ovqF+OyPnrkhX9yIi0oX03jzpi7RSJSIiIhKAgioRERGRABRUiYiIiASgoEpEREQkAAVVIiIiIgEoqBIRkS5TWVlJfn4+eXl5lJWVtcjfuHEjkyZNIjs7m3nz5rXIj8fjHHHEEZx88smNaVdeeSXjx49nwoQJnHDCCbzyyitdug8iHbXHoMrM3MzuSNrONLNtZnZ/tH2Vmc1pVmeLmQ0NP1wREdlXxONxZs+ezcqVK6murmbp0qVUV1c3KTNkyBAWLFjAnDlzWm3jpptuoqCgoEna5Zdfzvr161m7di0nn3wy11xzTZftg0hndGSl6l1grJntF21PAeq7bkgiItIbrF69mry8PEaNGkVWVhYlJSVUVFQ0KZOTk0NxcTH9+/dvUb+uro4VK1Zw4YUXNkkfOHBg4/d3330XM+uaHRDppI5e/lsJ7H6S2wxgadcMR0REeov6+npyc3Mbt2OxGPX1Hf83+WWXXcaPf/xjMjJa/q/q+9//Prm5uSxZskQrVdJjdDSoKgdKzOxTwHjgr3vTmZldZGZVZlYVf2/73jQhIiL7CHdvkdbRVaX777+fnJwcjjrqqFbzr7vuOmpra5k5cyYLFy5MaZwioXQoqHL39cBIEqtUf2ie3Va1Vtq5xd2L3L2o3/6DOjNOERHZx8RiMWpraxu36+rqGD58eIfq/vnPf2b58uWMHDmSkpISVq1axTe+8Y0W5b7+9a9zzz33BBuzSCo68+u/5cA8Wl76exM4sFnaAcDbez8sERHZ1xUXF1NTU8PmzZvZuXMn5eXlTJ8+vUN1r7/+eurq6tiyZQvl5eV8+ctf5s477wSgpqamsdzy5csZM2ZMl4xfpLM680LlW4Ht7r7BzI5LSn8cWGJmZe6+w8xOB9a5ezzgOEVEZB+TmZnJwoULmTp1KvF4nNLSUgoLC1m0aBEAs2bNYuvWrRQVFdHQ0EBGRgbz58+nurq6yc3ozc2dO5dNmzaRkZHBoYce2tieSLpZa9e8mxQwe8fdBzRLOw6Y4+4nR9v/DlxM4pLf68Asd3+pvXazh432YefO3+uBi4hIz7WlbNqeC4nsQ8xsjbsXtVdmjytVzQOqKO1R4NGk7V8Cv+z8EEVERER6Bz1RXURERCQABVUiIiIiASioEhEREQlAQZWIiIhIAJ15pEJQ40YMokq/DhEREZFeQitVIiIiIgEoqBIREREJQEGViIiISAAKqkREREQCSNuN6hvqtzNy7op0dS8i0qfotTEiXU8rVSIiIiIBKKgSERERCUBBlYiIiEgACqpEREREAlBQJSIiIhKAgioRkT6ksrKS/Px88vLyKCsra5G/ceNGJk2aRHZ2NvPmzWtM/+CDDzj66KP5whe+QGFhIT/84Q8b8y6//HLGjBnD+PHjOe2003j77be7Y1dEepwgQZWZxcyswsxqzOxFM7vJzLJCtC0iImHE43Fmz57NypUrqa6uZunSpVRXVzcpM2TIEBYsWMCcOXOapGdnZ7Nq1SrWrVvH2rVrqays5KmnngJgypQpPPfcc6xfv57DDjuM66+/vtv2SaQnSTmoMjMDlgH3ufto4DBgAHBdqm2LiEg4q1evJi8vj1GjRpGVlUVJSQkVFRVNyuTk5FBcXEz//v2bpJsZAwYMAGDXrl3s2rWLxOkfTjjhBDIzE489POaYY6irq+uGvRHpeUKsVH0Z+MDdbwNw9zjwv4FSM9s/QPsiIhJAfX09ubm5jduxWIz6+voO14/H40yYMIGcnBymTJnCxIkTW5S59dZbOfHEE4OMV2RfEyKoKgTWJCe4ewPwMpAXoH0REQnA3Vuk7V5t6oh+/fqxdu1a6urqWL16Nc8991yT/Ouuu47MzExmzpyZ8lhF9kUhgioDWv5NbSXdzC4ysyozq4q/tz1A1yIi0lGxWIza2trG7bq6OoYPH97pdgYPHsxxxx1HZWVlY9rtt9/O/fffz5IlSzoVqIn0JiGCqueBouQEMxsI5AIvJqe7+y3uXuTuRf32HxSgaxER6aji4mJqamrYvHkzO3fupLy8nOnTp3eo7rZt2xp/1ff+++/z0EMPMWbMGCDxi8IbbriB5cuXs//+uutD+q4QL1R+GCgzs3Pc/ddm1g+4Efh/7v5egPZFRCSAzMxMFi5cyNSpU4nH45SWllJYWMiiRYsAmDVrFlu3bqWoqIiGhgYyMjKYP38+1dXVvPrqq5x77rnE43E+/vhjzjrrLE4++WQALrnkEj788EOmTJkCJG5W392mSF9irV1j73QjZrnAfwNjSKx+/QGY4+4ftlUne9hoH3bu/JT7FhGRPdtSNi3dQxDZp5nZGncvaq9MiJUq3L0W+FqItkRERET2RXqiuoiIiEgACqpEREREAlBQJSIiIhKAgioRERGRABRUiYiIiAQQ5Nd/e2PciEFU6Se+IiIi0ktopUpEREQkAAVVIiIiIgEoqBIREREJQEGViIiISABpu1F9Q/12Rs5dka7uRUT2SXqHn0jPpZUqERERkQAUVImIiIgEoKBKREREJAAFVSIiIiIBKKgSERERCUBBlYjIPqayspL8/Hzy8vIoKytrkb9x40YmTZpEdnY28+bNa0yvra3l+OOPp6CggMLCQm666abGvLPPPpsJEyYwYcIERo4cyYQJE7pjV0R6lWCPVDCz04BlQIG7bwzVroiIfCIejzN79mwefPBBYrEYxcXFTJ8+ncMPP7yxzJAhQ1iwYAH33Xdfk7qZmZnceOONHHnkkezYsYOjjjqKKVOmcPjhh/Pb3/62sdx//Md/MGjQoO7aJZFeI+RK1QzgT0BJwDZFRCTJ6tWrycvLY9SoUWRlZVFSUkJFRUWTMjk5ORQXF9O/f/8m6cOGDePII48E4IADDqCgoID6+vomZdydu+66ixkzZnTtjoj0QkGCKjMbAPwLcAEKqkREukx9fT25ubmN27FYrEVg1BFbtmzh2WefZeLEiU3Sn3jiCT7zmc8wevTolMcq0teEWqk6Fah09xeAf5rZka0VMrOLzKzKzKri720P1LWISN/h7i3SzKxTbbzzzjucccYZzJ8/n4EDBzbJW7p0qVapRPZSqKBqBlAefS+Ptltw91vcvcjdi/rtr+v1IiKdFYvFqK2tbdyuq6tj+PDhHa6/a9cuzjjjDGbOnMnpp5/eJO+jjz5i2bJlnH322cHGK9KXpHyjupkdBHwZGGtmDvQD3Myu8Nb+SSUiInutuLiYmpoaNm/ezIgRIygvL+c3v/lNh+q6OxdccAEFBQV897vfbZH/0EMPMWbMGGKxWOhhi/QJIX79dybwa3f/990JZvYY8EXgiQDti4hIJDMzk4ULFzJ16lTi8TilpaUUFhayaNEiAGbNmsXWrVspKiqioaGBjIwM5s+fT3V1NevXr+eOO+5g3LhxjY9M+NGPfsRJJ50EQHl5uS79iaTAUl1MMrNHgTJ3r0xK+zaJRyt8q6162cNG+7Bz56fUt4hIX7OlbFq6hyDSJ5nZGncvaq9MyitV7n5cK2kLUm1XREREZF+iJ6qLiIiIBKCgSkRERCQABVUiIiIiASioEhEREQkg2AuVO2vciEFU6VcsIiIi0ktopUpEREQkAAVVIiIiIgEoqBIREREJQEGViIiISABpu1F9Q/12Rs5dka7uRUS6nF4pI9K3aKVKREREJAAFVSIiIiIBKKgSERERCUBBlYiIiEgACqpEREREAlBQJSLShSorK8nPzycvL4+ysrIW+Rs3bmTSpElkZ2czb968JnmlpaXk5OQwduzYJunr1q1j0qRJjBs3jq997Ws0NDR06T6ISMekHFSZmZvZHUnbmWa2zczuT7VtEZF9WTweZ/bs2axcuZLq6mqWLl1KdXV1kzJDhgxhwYIFzJkzp0X98847j8rKyhbpF154IWVlZWzYsIHTTjuNn/zkJ122DyLScSFWqt4FxprZftH2FKA+QLsiIvu01atXk5eXx6hRo8jKyqKkpISKioomZXJyciguLqZ///4t6k+ePJkhQ4a0SN+0aROTJ08GYMqUKdxzzz1dswMi0imhLv+tBHY/5W4GsDRQuyIi+6z6+npyc3Mbt2OxGPX1qf+bc+zYsSxfvhyAu+++m9ra2pTbFJHUhQqqyoESM/sUMB74a6B2RUT2We7eIs3MUm731ltv5ec//zlHHXUUO3bsICsrK+U2RSR1QV5T4+7rzWwkiVWqP7RVzswuAi4C6Dfw4BBdi4j0WLFYrMkqUl1dHcOHD0+53TFjxvDAAw8A8MILL7BihV75JdIThPz133JgHu1c+nP3W9y9yN2L+u0/KGDXIiI9T3FxMTU1NWzevJmdO3dSXl7O9OnTU2739ddfB+Djjz/m2muvZdasWSm3KSKpCxlU3Qpc4+4bArYpIrLPyszMZOHChUydOpWCggLOOussCgsLWbRoEYsWLQJg69atxGIxfvrTn3LttdcSi8UaH5EwY8YMJk2axKZNm4jFYixevBiApUuXcthhhzFmzBiGDx/O+eefn7Z9FJFPWGvX/DvVgNk77j6gWdpxwBx3P7mtetnDRvuwc+en1LeISE+2pWzanguJyD7BzNa4e1F7ZVK+p6p5QBWlPQo8mmrbIiIiIvsKPVFdREREJAAFVSIiIiIBKKgSERERCUBBlYiIiEgACqpEREREAgjyRPW9MW7EIKr0c2MRERHpJbRSJSIiIhKAgioRERGRABRUiYiIiASgoEpEREQkgLTdqL6hfjsj565IV/ciIkHo/X4isptWqkREREQCUFAlIiIiEoCCKhEREZEAFFSJiIiIBKCgSkRERCQABVUiIimqrKwkPz+fvLw8ysrKWuRv3LiRSZMmkZ2dzbx585rklZaWkpOTw9ixY1vUu/nmm8nPz6ewsJArrriiy8YvImEEeaSCmR0EPBxtHgLEgW3R9tHuvjNEPyIiPU08Hmf27Nk8+OCDxGIxiouLmT59OocffnhjmSFDhrBgwQLuu+++FvXPO+88LrnkEs4555wm6Y888ggVFRWsX7+e7OxsXn/99a7eFRFJUZCVKnd/090nuPsEYBHws93bCqhEpDdbvXo1eXl5jBo1iqysLEpKSqioqGhSJicnh+LiYvr379+i/uTJkxkyZEiL9F/84hfMnTuX7OzsxjZEpGfT5T8RkRTU19eTm5vbuB2Lxaivr0+53RdeeIEnnniCiRMn8qUvfYmnn3465TZFpGt16xPVzewi4CKAfgMP7s6uRUS6hLu3SDOzlNv96KOPeOutt3jqqad4+umnOeuss3jppZeCtC0iXaNbV6rc/RZ3L3L3on77D+rOrkVEukQsFqO2trZxu66ujuHDhwdp9/TTT8fMOProo8nIyOCNN95IuV0R6Tq6/CcikoLi4mJqamrYvHkzO3fupLy8nOnTp6fc7qmnnsqqVauAxKXAnTt3MnTo0JTbFZGuk7YXKouI9AaZmZksXLiQqVOnEo/HKS0tpbCwkEWLFgEwa9Ystm7dSlFREQ0NDWRkZDB//nyqq6sZOHAgM2bM4NFHH+WNN94gFotx9dVXc8EFF1BaWkppaSljx44lKyuL22+/XZf+RHo4a+1+gJQaNLsKeMfd57VXLnvYaB927vygfYuIdLctZdPSPQQR6QZmtsbdi9orE3ylyt2vCt2miIiISE+ne6pEREREAlBQJSIiIhKAgioRERGRABRUiYiIiASQtkcqjBsxiCr9akZERER6Ca1UiYiIiASgoEpEREQkAAVVIiIiIgEoqBIREREJQEGViIiISAAKqkREREQCUFAlIiIiEoCCKhEREZEAzN3T07HZDmBTWjrvWYYCb6R7ED2A5iFB85CgeUjQPCRoHhI0DwnpmodD3f3g9gqk7YnqwCZ3L0pj/z2CmVVpHjQPu2keEjQPCZqHBM1DguYhoSfPgy7/iYiIiASgoEpEREQkgHQGVbekse+eRPOQoHlI0DwkaB4SNA8JmocEzUNCj52HtN2oLiIiItKb6PKfiIiISABBgioz+6qZbTKzv5vZ3FbyzcwWRPnrzezIPdU1syFm9qCZ1UR/HhhirF1pb+fBzHLN7BEz+5uZPW9m30mqc5WZ1ZvZ2uhzUnfu095I8XjYYmYbon2tSkrvS8dDftJ/77Vm1mBml0V5vfF4GGNmT5rZh2Y2pyN1e+nx0Oo89MHzQ3vHQ186P7R1PPS188PM6Py43sz+YmZf2FPdtB4P7p7SB+gHvAiMArKAdcDhzcqcBKwEDDgG+Oue6gI/BuZG3+cCN6Q61q78pDgPw4Ajo+8HAC8kzcNVwJx07193zEOUtwUY2kq7feZ4aKWdrSSej9Jbj4ccoBi4Lnnf+uD5oa156Gvnh1bnIcrrS+eHNuehWTu9/fxwLHBg9P1Eenj8EGKl6mjg7+7+krvvBMqBU5qVOQX4tSc8BQw2s2F7qHsKcHv0/Xbg1ABj7Up7PQ/u/qq7PwPg7juAvwEjunPwAaVyPLSnzxwPzcp8BXjR3f/R9UPuEnucB3d/3d2fBnZ1om6vOx7amoe+dn5o53hoT585HprpC+eHv7j7W9HmU0CsA3XTdjyECKpGALVJ23W0/AvfVpn26n7G3V+FxEmFRNTek6UyD43MbCRwBPDXpORLoqXPW/eBZe1U58GBB8xsjZldlFSmTx4PQAmwtFlabzse9qZubzwe9qiPnB/a05fODx3R184PF5BY3d9T3bQdDyGCKmslrflPCtsq05G6+4pU5iGRaTYAuAe4zN0bouRfAJ8HJgCvAjemPNKuleo8/Iu7H0limXe2mU0OObhuFOJ4yAKmA3cn5ffG46Er6vY0Ke9LHzo/tKcvnR/ab6CPnR/M7HgSQdX/6Wzd7hQiqKoDcpO2Y8ArHSzTXt3Xdl8Kif58PcBYu1Iq84CZ9Sdxwlzi7st2F3D319w97u4fA/9DYsmzJ0tpHtx995+vA/fyyf72qeMhciLwjLu/tjuhlx4Pe1O3Nx4Pbepj54c29bHzw570mfODmY0HfgWc4u5vdqBu2o6HEEHV08BoM/tcFDmXAMublVkOnGMJxwDboyW59uouB86Nvp8LVAQYa1fa63kwMwMWA39z958mV2h2j81pwHNdtwtBpDIPnzazAwDM7NPACXyyv33meEjKn0Gzpf1eejzsTd3eeDy0qg+eH1rVB88Pe9Inzg9m9llgGfBNd3+hg3XTdzyEuNudxK+YXiBxJ/73o7RZwKzouwE/j/I3AEXt1Y3SDwIeBmqiP4eEGGtXfvZ2HoAvkli2XA+sjT4nRXl3RGXXkzhQhqV7P7twHkaR+AXHOuD5vno8RHn7A28Cg5q12RuPh0NI/KuzAXg7+j6wrbq9+HhodR764PmhrXnoa+eH9v5e9KXzw6+At5KO/ar26qb7eNAT1UVEREQC0BPVRURERAJQUCUiIiISgIIqERERkQAUVImIiIgEoKBKREREJAAFVSKCmcXtkzfbr41eh9JjmNnvzGxU9H2LmQ3txr4Hm9nF3dhflpk9bmaZ3dWniIShoEpEAN539wlJny1d3aGZ9etguUKgn7u/1MVDaq3vfsBgoNuCKk+8HPZh4Ozu6lNEwlBQJSIdYmbfNrPq6GWt5VHaADO7zcw2ROlnROkzorTnzOyGpDbeMbNrzOyvwCQz+4aZrY5Wx37ZRqA1k1aeiGxmI81so5n9KupniZn9m5n92cxqzOzoqNxVZnaHma2K0v9XlG5m9pOo7gYzOztKP87MHjGz35B4kGIZ8PlojD+J9vlhM3smqndK0nj+Zmb/Y2bPm9kDZrZflJdnZg+Z2bqo3uej9MvN7Olo7q5O2r37ov0WkX1Jup+mqo8++qT/A8T55InF97ZR5hUgO/o+OPrzBmB+UpkDgeHAy8DBQCawCjg1ynfgrOh7AfB7oH+0/d/AOa30+xgwLml7CzAUGAl8BIwj8Q/ENcCtJJ5UfwpwX1T+KhJP4d4vqlcbjfEM4EGgH/CZaMzDgOOAd4HPRfVHAs8l9Z/JJ0+2Hgr8Pepz93gmRHl3Ad+Ivv8VOC36/ikST8Q+AbglqpsB3A9Mjsr0A7al+7jQRx99OvfRNXsRgejy3x7KrAeWmNl9JFZSAP6NxDu3AHD3t8xsMvCou28DMLMlwOSoTpzEi4EBvgIcBTydeL0d+9H6i0+HAdvaGNNmd98Q9fM88LC7u5ltIBHk7Fbh7u8D75vZIyReNPtFYKm7x0m8gPUxoJjEa0FWu/vmNvo04EfRfn4MjCARlO0ez9ro+xpgZPS+uhHufm80Rx9E4z2BRGD1bFR+ADAaeNzd42a208wOcPcdbYxDRHoYBVUi0iozuw04AnjF3U8CppEIjqYDV0b3OhmJ1acmVdtp9oMoiNld7nZ3/94ehvI+idWd1nyY9P3jpO2PaXp+az5G38M4320nbyaJVbij3H2XmW1JGl/yeOIkAsW2+jHgenf/ZRv52cAH7YxDRHoY3VMlIq1y9/M9cdP6SWaWAeS6+yPAFSRu3h4APABcsruOmR1I4lLXl8xsaHSP1AwSl/Caexg408xyorpDzOzQVsr9DchLcXdOMbNPmdlBJC7vPQ08DpxtZv3M7GASAePqVuruAA5I2h4EvB4FVMcDrY25kbs3AHVmdiqAmWWb2f7AH4FSMxsQpY9ImouDSFz+27XXeywi3U5BlYh0RD/gzuiy2rPAz9z9beBa4MDoZu91wPHu/irwPeAREvcyPePuLW40d/dq4AfAA2a2nsT9TcNa6XsFiUAoFaujdp4C/svdXwHuJXFJcx2J+76ucPetrYzzTeDP0T7+BFgCFJlZFYlVq40d6P+bwLej/fwLcIi7PwD8Bngymtff8Unwdjzwh73eWxFJC3NvviouItJzRL+gewT4l6RLh52pfxXwjrvPCz22rmJmy4DvufumdI9FRDpOK1Ui0qNFN5j/kMQN4b2emWWR+OWiAiqRfYxWqkREREQC0EqViIiISAAKqkREREQCUFAlIiIiEoCCKhEREZEAFFSJiIiIBKCgSkRERCSA/w9zy54joKOjwAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.783021308980213\n",
      "Variance of Accuracy: 0.0001787088004373923\n",
      "Standard Deviation of Accuracy: 0.013368201092046466\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#df = pd.read_excel('G:\\\\data_all\\\\mathoverflow_c2a\\\\all_data.xlsx')\n",
    "df = pd.read_excel('G:\\\\data_all\\\\mathoverflow_c2q\\\\all_data.xlsx')\n",
    "\n",
    "num_iterations = 100\n",
    "\n",
    "accuracy_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "feature_importances = []\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    positive_samples = df[df['leibie'] == 1]\n",
    "    negative_samples = df[df['leibie'] == 0].sample(n=len(positive_samples), replace=True)\n",
    "    balanced_data = pd.concat([positive_samples, negative_samples])\n",
    "    #columns_to_select = ['N', 'R', 'O', 'K', 'M', 'T', 'MU', 'A'] \n",
    "    #Each time we remove the feature with the highest ranking importance, \n",
    "    #we record the accuracy offline, one by one, until all the features have been removed, and we get the sensitivity level\n",
    "    columns_to_select = ['R', 'O', 'K', 'M', 'T', 'MU', 'A'] \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        balanced_data[columns_to_select], \n",
    "        balanced_data['leibie'], test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "    xgb_model = XGBClassifier()\n",
    "    grid_search = GridSearchCV(xgb_model, param_grid, cv=3, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    model = XGBClassifier(**best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    recall_scores.append(recall_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "\n",
    "    feature_importances.append(model.feature_importances_)\n",
    "\n",
    "feature_names = columns_to_select\n",
    "\n",
    "mean_feature_importances = np.mean(feature_importances, axis=0)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': mean_feature_importances\n",
    "})\n",
    "\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, len(importance_df) / 2)) \n",
    "bars = plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "\n",
    "for bar in bars:\n",
    "    plt.text(\n",
    "        bar.get_width(),     \n",
    "        bar.get_y() + bar.get_height() / 2,  \n",
    "        f'{bar.get_width():.3f}',  \n",
    "        va='center'  \n",
    "    )\n",
    "\n",
    "plt.xlabel('F-score (Importance)')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()\n",
    "\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "variance_accuracy = np.var(accuracy_scores)\n",
    "std_dev_accuracy = np.std(accuracy_scores)\n",
    "\n",
    "print(\"Mean Accuracy:\", mean_accuracy)\n",
    "print(\"Variance of Accuracy:\", variance_accuracy)\n",
    "print(\"Standard Deviation of Accuracy:\", std_dev_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc63a83d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cb8c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
